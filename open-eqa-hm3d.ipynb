{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55aef96e",
   "metadata": {},
   "source": [
    "# OpenEQA QuickStart\n",
    "\n",
    "Welcome! This notebook is intended to provide a quick-start guide to OpenEQA and its workflow. Before you can start using this notebook, please perform a few basic installation steps.\n",
    "\n",
    "## Packages and Dependencies\n",
    "- Install [openeqa](https://github.com/facebookresearch/open-eqa/blob/main/setup.py) and [required packages](https://github.com/facebookresearch/open-eqa/blob/main/requirements.txt).\n",
    "- You don't need the full dataset to run this notebook and get started. But we highly encourage you to look at instructions to [download the data](https://github.com/facebookresearch/open-eqa/blob/main/data/README.md) and get started early! We realize getting access to the dataset has a few hoops to jump through and thus we provide this repo with a small \"demo dataset\" to give you a flavor of the benchmark.\n",
    "- Get API access keys ready for different models (e.g. GPT, Claude) or host the model on your end (e.g. LLaMA, Mixtral)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae3c287",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "Let's first start by visualizing a demo dataset generated from a [public HM3D scene](https://aihabitat.org/datasets/hm3d/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c30848ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = '<OPENAI_API_KEY>'\n",
    "os.environ['OPENAI_AZURE_DEPLOYMENT'] = '1'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Video, Image\n",
    "\n",
    "Video(\"data/videos/hm3d-v0/000-hm3d-BFRyYbPCCPE-0.mp4\", embed=True, width=480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc685abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from openeqa.utils.demo_utils import (\n",
    "    decode_frames_from_video_path,\n",
    "    get_equally_spaced_frames,\n",
    ")\n",
    "\n",
    "frames_per_traj = 15\n",
    "\n",
    "# get the frames from the video\n",
    "frames = decode_frames_from_video_path(\"data/videos/hm3d-v0/000-hm3d-BFRyYbPCCPE-0.mp4\")\n",
    "\n",
    "# extract equally spaced frames\n",
    "frames = get_equally_spaced_frames(frames, frames_per_traj)\n",
    "\n",
    "# display a few frames\n",
    "_, axs = plt.subplots(1, 4, figsize=(12, 12))\n",
    "for img, ax in zip(frames[::4], axs):\n",
    "    ax.axis(\"off\")\n",
    "    ax.imshow(img)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540466eb",
   "metadata": {},
   "source": [
    "## Load the Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de0401d9-cf4b-4ca3-9476-867f051e6fa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#HM3D EQA : 557 instances\n",
      "\n",
      "Top-5 Samples:\n",
      "[\n",
      "  {\n",
      "    \"question\": \"What is the white object on the wall above the TV?\",\n",
      "    \"answer\": \"Air conditioning unit\",\n",
      "    \"category\": \"object recognition\",\n",
      "    \"question_id\": \"f2e82760-5c3c-41b1-88b6-85921b9e7b32\",\n",
      "    \"episode_history\": \"hm3d-v0/000-hm3d-BFRyYbPCCPE\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"What material is the ceiling in the living room?\",\n",
      "    \"answer\": \"Wood panel\",\n",
      "    \"category\": \"attribute recognition\",\n",
      "    \"question_id\": \"7447d782-d1a7-4c87-86dc-b5eafc5a0f76\",\n",
      "    \"episode_history\": \"hm3d-v0/000-hm3d-BFRyYbPCCPE\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"What color is the staircase railing?\",\n",
      "    \"answer\": \"Brown\",\n",
      "    \"category\": \"attribute recognition\",\n",
      "    \"question_id\": \"e2ccf6f4-22a9-47d1-ab8d-a05a13435b82\",\n",
      "    \"episode_history\": \"hm3d-v0/000-hm3d-BFRyYbPCCPE\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"What is in between the two picture frames on the blue wall in the living room?\",\n",
      "    \"answer\": \"The TV\",\n",
      "    \"category\": \"spatial understanding\",\n",
      "    \"question_id\": \"c841bb52-1cec-46d7-bb83-8c99b5c66fa8\",\n",
      "    \"episode_history\": \"hm3d-v0/000-hm3d-BFRyYbPCCPE\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Is there room on the dining table to eat?\",\n",
      "    \"answer\": \"Yes\",\n",
      "    \"category\": \"spatial understanding\",\n",
      "    \"question_id\": \"79344680-6b45-4531-8789-ad0f5ef85b3b\",\n",
      "    \"episode_history\": \"hm3d-v0/000-hm3d-BFRyYbPCCPE\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "eqa_data = json.load(open(\"data/open-eqa-v0.json\"))\n",
    "hm3d_eqa_data = list(filter(lambda x: 'hm3d-v0' in x['episode_history'], eqa_data))\n",
    "print(f'#HM3D EQA : {len(hm3d_eqa_data)} instances')\n",
    "print('\\nTop-5 Samples:')\n",
    "print(json.dumps(hm3d_eqa_data[:5], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dfe364",
   "metadata": {},
   "source": [
    "## Ask the Model\n",
    "\n",
    "Let's start with a text-only (or blind) LLM. We'll use GPT-4 for illustration, but we encourage you to try out your own model!\n",
    "\n",
    "First set the OPENAI_API_KEY environment variable to your own openai api key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f535023b-9660-40ec-bd37-700b34d4bf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████████████████████▊                                                                                                                                               | 1/5 [00:01<00:07,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the white object on the wall above the TV?\n",
      "A: A clock\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████████████████████████████████████▌                                                                                                           | 2/5 [00:02<00:03,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What material is the ceiling in the living room?\n",
      "A: Drywall\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                       | 3/5 [00:03<00:02,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What color is the staircase railing?\n",
      "A: brown\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                   | 4/5 [00:05<00:01,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is in between the two picture frames on the blue wall in the living room?\n",
      "A: a clock\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Is there room on the dining table to eat?\n",
      "A: Yes\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "assert \"OPENAI_API_KEY\" in os.environ\n",
    "if os.environ.get('OPENAI_AZURE_DEPLOYMENT') == '1':\n",
    "    from openeqa.baselines.gpt4_azure import ask_question as ask_blind_gpt4\n",
    "else:\n",
    "    from openeqa.baselines.gpt4 import ask_question as ask_blind_gpt4\n",
    "\n",
    "verbose = True\n",
    "\n",
    "for item in tqdm(hm3d_eqa_data[:5]):\n",
    "    q = item[\"question\"]\n",
    "    a = item[\"answer\"]\n",
    "    blind_gpt4_a = ask_blind_gpt4(\n",
    "        question=q,\n",
    "        openai_model=\"gpt-4o\",\n",
    "    )\n",
    "    item[\"blind_gpt4_answer\"] = blind_gpt4_a\n",
    "    if verbose:\n",
    "        # print the question and the model's answer\n",
    "        print(\"Q: {}\\nA: {}\\n\".format(q, blind_gpt4_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265a8b8b-fc30-445a-ac3a-c07fe94fb5c0",
   "metadata": {},
   "source": [
    "Next, lets try a multi-modal LLM. We'll use GPT-4V, but other models are implemented in the open-eqa codebase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f67d97e3-bab0-46c1-8e43-0a2d834aa739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████████████████████▊                                                                                                                                               | 1/5 [00:06<00:24,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the white object on the wall above the TV?\n",
      "A: The white object on the wall above the TV is an air conditioning unit.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████████████████████████████████████▌                                                                                                           | 2/5 [00:12<00:18,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What material is the ceiling in the living room?\n",
      "A: The ceiling in the living room appears to be made of wood.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                       | 3/5 [00:19<00:13,  6.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What color is the staircase railing?\n",
      "A: The staircase railing is dark brown.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                   | 4/5 [00:29<00:07,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is in between the two picture frames on the blue wall in the living room?\n",
      "A: There is an air conditioning unit in between the two picture frames on the blue wall in the living room.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:36<00:00,  7.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Is there room on the dining table to eat?\n",
      "A: Yes, there appears to be room on the dining table to eat. The table is visible in the images and seems to have enough space for dining.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "assert \"OPENAI_API_KEY\" in os.environ\n",
    "import glob\n",
    "\n",
    "if os.environ.get('OPENAI_AZURE_DEPLOYMENT') == '1':\n",
    "    from openeqa.baselines.gpt4o_azure import ask_question as ask_gpt4o\n",
    "else:\n",
    "    from openeqa.baselines.gpt4v import ask_question as ask_gpt4o\n",
    "\n",
    "verbose = True\n",
    "\n",
    "for item in tqdm(hm3d_eqa_data[:5]):\n",
    "    q = item[\"question\"]\n",
    "    image_paths = sorted(glob.glob(f\"data/frames/{item['episode_history']}/*.png\"))\n",
    "    filt_image_paths = []\n",
    "    for depth_img, rgb_img in zip(image_paths[::16], image_paths[1::16]):\n",
    "        filt_image_paths.append(depth_img)\n",
    "        filt_image_paths.append(rgb_img)\n",
    "    \n",
    "    gpt4o_a = ask_gpt4o(\n",
    "        question=q,\n",
    "        image_paths=filt_image_paths,\n",
    "        openai_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        openai_model=\"gpt-4o\",\n",
    "    )\n",
    "    item[\"gpt4o_answer\"] = gpt4o_a\n",
    "    if verbose:\n",
    "        # print the question and the model's answer\n",
    "        print(\"Q: {}\\nA: {}\\n\".format(q, gpt4o_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb42222f",
   "metadata": {},
   "source": [
    "## Evaluate the Answers w/ LLM-Match\n",
    "\n",
    "Finally, we'll evaluate the open-vocabulary answers produced by an LLMs or multi-modal LLMs by comparing against the ground truth answer(s) using LLM-Match, which uses a rating of 1-5 to score answers. See the paper for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa8caf8a-d32b-41cc-8761-903d5ae6dcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.62it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.47it/s]\n"
     ]
    }
   ],
   "source": [
    "assert \"OPENAI_API_KEY\" in os.environ\n",
    "from openeqa.evaluation.llm_match import get_llm_match_score\n",
    "\n",
    "# evaluate GPT-4 answers\n",
    "for item in tqdm(hm3d_eqa_data[:5]):\n",
    "    c = get_llm_match_score(\n",
    "        question=item[\"question\"],\n",
    "        answer=item[\"answer\"],\n",
    "        prediction=item[\"blind_gpt4_answer\"],\n",
    "    )  # correctness score (see Eq. 1 and Table 2 in paper)\n",
    "    item[\"blind_gpt4_score\"] = c\n",
    "\n",
    "# evaluate GPT-4o answers\n",
    "for item in tqdm(hm3d_eqa_data[:5]):\n",
    "    c = get_llm_match_score(\n",
    "        question=item[\"question\"],\n",
    "        answer=item[\"answer\"],\n",
    "        prediction=item[\"gpt4o_answer\"],\n",
    "    )  # correctness score (see Eq. 1 and Table 2 in paper)\n",
    "    item[\"gpt4o_score\"] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0e2597e-94f8-4cfe-8340-453de4c9138c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"question\": \"What is the white object on the wall above the TV?\",\n",
      "    \"answer\": \"Air conditioning unit\",\n",
      "    \"category\": \"object recognition\",\n",
      "    \"question_id\": \"f2e82760-5c3c-41b1-88b6-85921b9e7b32\",\n",
      "    \"episode_history\": \"hm3d-v0/000-hm3d-BFRyYbPCCPE\",\n",
      "    \"blind_gpt4_answer\": \"A clock\",\n",
      "    \"gpt4o_answer\": \"The white object on the wall above the TV is an air conditioning unit.\",\n",
      "    \"blind_gpt4_score\": 1,\n",
      "    \"gpt4o_score\": 5\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"What material is the ceiling in the living room?\",\n",
      "    \"answer\": \"Wood panel\",\n",
      "    \"category\": \"attribute recognition\",\n",
      "    \"question_id\": \"7447d782-d1a7-4c87-86dc-b5eafc5a0f76\",\n",
      "    \"episode_history\": \"hm3d-v0/000-hm3d-BFRyYbPCCPE\",\n",
      "    \"blind_gpt4_answer\": \"Drywall\",\n",
      "    \"gpt4o_answer\": \"The ceiling in the living room appears to be made of wood.\",\n",
      "    \"blind_gpt4_score\": 1,\n",
      "    \"gpt4o_score\": 5\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"What color is the staircase railing?\",\n",
      "    \"answer\": \"Brown\",\n",
      "    \"category\": \"attribute recognition\",\n",
      "    \"question_id\": \"e2ccf6f4-22a9-47d1-ab8d-a05a13435b82\",\n",
      "    \"episode_history\": \"hm3d-v0/000-hm3d-BFRyYbPCCPE\",\n",
      "    \"blind_gpt4_answer\": \"brown\",\n",
      "    \"gpt4o_answer\": \"The staircase railing is dark brown.\",\n",
      "    \"blind_gpt4_score\": 5,\n",
      "    \"gpt4o_score\": 5\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"What is in between the two picture frames on the blue wall in the living room?\",\n",
      "    \"answer\": \"The TV\",\n",
      "    \"category\": \"spatial understanding\",\n",
      "    \"question_id\": \"c841bb52-1cec-46d7-bb83-8c99b5c66fa8\",\n",
      "    \"episode_history\": \"hm3d-v0/000-hm3d-BFRyYbPCCPE\",\n",
      "    \"blind_gpt4_answer\": \"a clock\",\n",
      "    \"gpt4o_answer\": \"There is an air conditioning unit in between the two picture frames on the blue wall in the living room.\",\n",
      "    \"blind_gpt4_score\": 1,\n",
      "    \"gpt4o_score\": 1\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Is there room on the dining table to eat?\",\n",
      "    \"answer\": \"Yes\",\n",
      "    \"category\": \"spatial understanding\",\n",
      "    \"question_id\": \"79344680-6b45-4531-8789-ad0f5ef85b3b\",\n",
      "    \"episode_history\": \"hm3d-v0/000-hm3d-BFRyYbPCCPE\",\n",
      "    \"blind_gpt4_answer\": \"Yes\",\n",
      "    \"gpt4o_answer\": \"Yes, there appears to be room on the dining table to eat. The table is visible in the images and seems to have enough space for dining.\",\n",
      "    \"blind_gpt4_score\": 5,\n",
      "    \"gpt4o_score\": 4\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# print the final results\n",
    "print(json.dumps(hm3d_eqa_data[:5], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bc7b3ae-63af-4500-b254-e69653483c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "os.environ['OPENAI_AZURE_DEPLOYMENT'] = '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14983acc-696e-4766-a9d8-2899a04fce9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env-openeqa)",
   "language": "python",
   "name": "env_openeqa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
